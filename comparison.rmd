---
title: "Classifier Comparison"
author: "Rami Khalil"
date: "April 11, 2016"
output: html_document
---

```{r}
library(RWeka)
library(partykit)
library(dplyr)
library(knitr)
```

# 1. Initialization

We first read the initial dataset and swap two columns as to make the first variable the target of classification.

```{r}
sonar <- read.csv("sonar/sonar.all-data", header = FALSE) %>% rename(V1 = V61, V61 = V1)
```

Here we define our method that compares whole-dataset testing with 10-fold cross validation and our metrics calculation method.

```{r}
compareWith10Fold <- function(classifier) {
  entireDataSet <- summary(classifier)
  print(entireDataSet)
  print(calculateMetrics(entireDataSet))
  
  kfold <- evaluate_Weka_classifier(classifier, numFolds = 10)
  print(kfold)
  print(calculateMetrics(kfold))
}

calculateMetrics <- function(kfold) {
  TP <- kfold$confusionMatrix[1, 1]
  TN <- kfold$confusionMatrix[2, 2]
  FP <- kfold$confusionMatrix[2, 1]
  FN <- kfold$confusionMatrix[1, 2]
  
  accuracy  <- (TP + TN) / (TP + TN + FP + FN)
  precision <- TP / (TP + FP)
  recall    <- TP / (TP + FN)
  f1        <- 2 * precision * recall / (precision + recall)
  
  res <- list(Accuracy = accuracy, Precision = precision, Recall = recall, F1 = f1)
  return(res)
}
```

# 2. C4.5 Classifier

This section contains the results of training a C4.5 classifier using the entire dataset for both training and testing, and then results of training using 10-fold cross validation.

```{r}
J48(V1 ~ ., sonar) %>% compareWith10Fold()
```

# 3. Other Classifiers

Here we repeat the same procedure and output the same two results (with and without cross-validation) for each of the following classifiers:

## Random Forest

```{r}
RF <- make_Weka_classifier("weka/classifiers/trees/RandomForest")
RF(V1 ~ ., data = sonar) %>% compareWith10Fold()
```

## Support Vector Machine

```{r}
SMO(V1 ~ ., data = sonar) %>% compareWith10Fold()
```

## Naive Bayes Classifier

```{r}
NB <- make_Weka_classifier("weka/classifiers/bayes/NaiveBayes")
NB(V1 ~ ., data = sonar) %>% compareWith10Fold()
```

## Neural Network

```{r}
MLP <- make_Weka_classifier("weka/classifiers/functions/MultilayerPerceptron")
MLP(V1 ~ ., data = sonar) %>% compareWith10Fold()
```

## Bagging

```{r}
C45Bagging <- function(form, data, options = list(model = TRUE)) {
  return(Bagging(formula(form), data, control = Weka_control(W = "weka.classifiers.trees.J48"), options = options))
}
C45Bagging(V1 ~ ., data = sonar) %>% compareWith10Fold()
```

## Boosting

```{r}
AdaBoostM1(V1 ~ ., data = sonar) %>% compareWith10Fold()
```

# 4. Satistically Significant Results

## Datasets

We proceed to load the remaining datasets and standardize the target classification variable to be named *V1*.

```{r}
hepatitis <- read.csv("hepatitis/hepatitis.data", header = FALSE) %>% mutate(V1 = as.factor(V1))
spect <- 
  rbind(read.csv("spect/SPECT.train", header = FALSE), read.csv("spect/SPECT.test", header = FALSE)) %>%
  mutate(V1 = as.factor(V1))
pima <- read.csv("pima/pima-indians-diabetes.data", header = FALSE) %>%
  rename(V1 = V9, V9 = V1) %>%
  mutate(V1 = as.factor(V1))

datasets <- list(sonar, hepatitis, spect, pima)
names(datasets) <- c("Sonar", "Hepatitis", "Spect", "Pima")
```

## Methods

In this section we define our methods for calculating the different metrics on a dataset for each algorithm we have used. We have ommitted the calculations for the multi-layer perceptron ANN since it is very slow. Uncommenting the below lines will produce the intended output, but will take a lot of time.

```{r}
testClassifiers <- function(dataset) {
  # The multi-layer perceptron classifier is very slow.
  # Uncomment both the algorithm and the name in the list to enable.
  # Will take ~1 minute to execute on a Core i7-5700HQ @ 2.7GHz
  algorithms <- c(
    J48,
    RF,
    SMO,
    NB,
    #MLP, # THIS IS VERY SLOW. UNCOMMENT AT YOUR OWN RISK!
    C45Bagging,
    AdaBoostM1)
  res <- c()
  for (func in algorithms) {
    subres <- c()
    for (i in 1:10) {
      subres <- c(
        subres, 
        list(
          calculateMetrics(
            evaluate_Weka_classifier(
              func(V1 ~ ., dataset, options = list(model = TRUE)),
              numFolds = 10))))
    }
    res <- c(res, list(subres))
  }
  names(res) <- c(
    "C4.5",
    "Random Forest",
    "SVM",
    "Naive Bayes",
    #"Neural Network", # THIS IS VERY SLOW. UNCOMMENT AT YOUR OWN RISK!
    "Bagging",
    "Boosting")
  
  res <- lapply(res, rbind_all)
  return(res)
}
```

Defined below is the method for performing pair-wise t-tests for every two algorithms for a metric on a given set of calculations.

```{r}
pairwiseTesting <- function(calcs) {
  cols <-c("Accuracy", "Precision", "Recall", "F1")
  inv <- lapply(cols, function(x) {
    sapply(calcs, "[[", i = x) %>% data.frame()
  })
  names(inv) <- cols

  lapply(inv, function(metricDF) {
    sapply(metricDF, function(var1) {
      sapply(metricDF, function(var2) {
        tmp <- t.test(var1, var2)
        paste(
          sprintf("%.3f", tmp$statistic[[1]]),
          sprintf("%.3f", tmp$p.value),
          sep = ",p=")
        #list(t = tmp$statistic[[1]], tmp$p.value)
      })
    }) %>% data.frame()
  })
}

kableMetrics <- function(results, caption) {
  results$Accuracy %>% kable(caption = paste(caption, "Accuracy")) %>% print()
  results$Precision %>% kable(caption = paste(caption, "Precision")) %>% print()
  results$Recall %>% kable(caption = paste(caption, "Recall")) %>% print()
  results$F1 %>% kable(caption = paste(caption, "F1 Score")) %>% print()
}
```

Below is the method to calculate the number of *wins* every algorithm has in a table. We say that an algorithm has one when the difference in means between it and another algorithm is in its favor with a p-value of < 0.05.

```{r}
winCalculation <- function(lst) {
  lapply(lst, function(dframe) {
    row <- sapply(dframe, function(col) {
      nums <- strsplit(as.character(col), ",p=")
      win <- sapply(nums, function(res) {
        tval <- as.numeric(res[[1]])
        pval <- as.numeric(res[[2]])
        subres <- (tval > 0) & (pval < 0.05)

        return (subres)
      })
      sum(win)
    }) %>% data.frame() %>% t() %>% data.frame()
    row.names(row) <- "Wins"
    row
  })
}
```

## Calcluations

Here we calculate our metrics 10 times for each (dataset, algorithm) pair and display the mean of the results.

```{r}
calculations <- lapply(datasets, testClassifiers)
```

```{r}
deepApply <- function(x, func = mean) {
  sapply(x, function(y) {
    sapply(y, func)
  })
}
metrics <- lapply(calculations, deepApply)

sapply(metrics, "[", i = "Accuracy", j = 1:6) %>% data.frame() %>% t() %>% kable(caption = "Accuracy")
sapply(metrics, "[", i = "Precision", j = 1:6) %>% data.frame() %>% t() %>% kable(caption = "Precision")
sapply(metrics, "[", i = "Recall", j = 1:6) %>% data.frame() %>% t() %>% kable(caption = "Recall")
sapply(metrics, "[", i = "F1", j = 1:6) %>% data.frame() %>% t() %>% kable(caption = "F1 Score")
```

## Comparisons

In this section we will perform pairwise t tests for each dataset.

### Sonar

```{r, results='asis'}
pwise.sonar <-pairwiseTesting(calculations$Sonar)
kableMetrics(pwise.sonar, "Sonar Dataset")
```

### Hepatitis

```{r, results='asis'}
pwise.hepatitis <- pairwiseTesting(calculations$Hepatitis)
kableMetrics(pwise.hepatitis, "Hepatitis Dataset")
```

### SPECT

```{r, results='asis'}
pwise.spect <- pairwiseTesting(calculations$Spect)
kableMetrics(pwise.spect, "Spect Dataset")
```

### Pima Indians

```{r, results='asis'}
pwise.pima <- pairwiseTesting(calculations$Pima)
kableMetrics(pwise.pima,"Pima Dataset")
```

## Interpretation

Here we present the win-calculations for each dataset and algorithm. The questions of who has the most wins can be answered using the below tables for each dataset and measure of efficacy. A clear and only winner is only apparent for each dataset when it accrues substantially more wins than the other algorithms it competes with.

### Sonar

```{r, results='asis'}
winCalculation(pwise.sonar) %>%
  kableMetrics("Sonar Dataset")
```

The Random Forest classifier clearly dominates wins in the sonar dataset across all measures.

### Hepatitis

```{r, results='asis'}
winCalculation(pwise.hepatitis) %>%
  kableMetrics("Hepatitis Dataset")
```

The hepatitis dataset is a close competition between the C4.5 and Naive Bayes classifiers. The C4.5 classifier performs better than all others in terms of accuracy, while the Naive Bayes classifier outperforms the others in terms of recall and F1 score. Precision appears to be equally contested by C4.5 and Bagging.

### SPECT

```{r, results='asis'}
winCalculation(pwise.spect) %>%
  kableMetrics("SPECT Dataset")
```

The Naive Bayes classifier outperforms all others in terms of recall, and contests the SVM in terms of F1 score, while accuracy and precision are equally contested by the Random Forest, Support Vector Machine and Bagging classifiers.

### Pima Indians

```{r, results='asis'}
winCalculation(pwise.pima) %>%
  kableMetrics("PIMA Dataset")
```

Here the SVM classifier appears to dominate all but precision, which appears to be contested by the Random Forst and Naive Bayes classifiers.