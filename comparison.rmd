---
title: "Classifier Comparison"
author: "Rami Khalil"
date: "April 11, 2016"
output: html_document
---

```{r}
library(RWeka)
library(dplyr)
library(knitr)
```

# 1. Initialization

We first read the initial dataset and swap two columns as to make the first variable the target of classification.

```{r}
sonar <- read.csv("sonar/sonar.all-data", header = FALSE) %>% rename(V1 = V61, V61 = V1)
```

Here we define a method that compares whole-dataset training & testing with 10-fold cross validation and another method for our metrics calculations.

```{r}
compareWith10Fold <- function(classifier) {
  entireDataSet <- summary(classifier)
  print(entireDataSet)
  print(calculateMetrics(entireDataSet))
  
  kfold <- evaluate_Weka_classifier(classifier, numFolds = 10)
  print(kfold)
  print(calculateMetrics(kfold))
}

calculateMetrics <- function(kfold) {
  TP <- kfold$confusionMatrix[1, 1]
  TN <- kfold$confusionMatrix[2, 2]
  FP <- kfold$confusionMatrix[2, 1]
  FN <- kfold$confusionMatrix[1, 2]
  
  accuracy  <- (TP + TN) / (TP + TN + FP + FN)
  precision <- TP / (TP + FP)
  recall    <- TP / (TP + FN)
  f1        <- 2 * precision * recall / (precision + recall)
  
  res <- list(Accuracy = accuracy, Precision = precision, Recall = recall, F1 = f1)
  return(res)
}
```

# 2. C4.5 Classifier

This section contains the results of training a C4.5 classifier using the entire dataset for both training and testing, and then results of using 10-fold cross validation.

```{r}
J48(V1 ~ ., sonar) %>% compareWith10Fold()
```

# 3. Other Classifiers

Here we repeat the same procedure (training/testing using entire dataset, then 10-fold cross-validation) for each of the following classifiers:

## Random Forest

```{r}
RF <- make_Weka_classifier("weka/classifiers/trees/RandomForest")
RF(V1 ~ ., data = sonar) %>% compareWith10Fold()
```

## Support Vector Machine

```{r}
SMO(V1 ~ ., data = sonar) %>% compareWith10Fold()
```

## Naive Bayes Classifier

```{r}
NB <- make_Weka_classifier("weka/classifiers/bayes/NaiveBayes")
NB(V1 ~ ., data = sonar) %>% compareWith10Fold()
```

## Neural Network

```{r}
MLP <- make_Weka_classifier("weka/classifiers/functions/MultilayerPerceptron")
MLP5 <- function(form, data, options = list(model = TRUE)) {
  # Faster training with only 5 epochs
  return(MLP(formula(form), data, control = Weka_control(N = 5), options = options))
}
MLP(V1 ~ ., data = sonar) %>% compareWith10Fold()
```

## Bagging

```{r}
C45Bagging <- function(form, data, options = list(model = TRUE)) {
  return(Bagging(formula(form), data, control = Weka_control(W = "weka.classifiers.trees.J48"), options = options))
}
C45Bagging(V1 ~ ., data = sonar) %>% compareWith10Fold()
```

## Boosting

```{r}
AdaBoostM1(V1 ~ ., data = sonar) %>% compareWith10Fold()
```

# 4. Satistically Significant Results

## Datasets

We proceed to load the remaining datasets and standardize the target classification variable to be named *V1*.

```{r}
hepatitis <- read.csv("hepatitis/hepatitis.data", header = FALSE) %>% mutate(V1 = as.factor(V1))
spect <- 
  rbind(read.csv("spect/SPECT.train", header = FALSE), read.csv("spect/SPECT.test", header = FALSE)) %>%
  mutate(V1 = as.factor(V1))
pima <- read.csv("pima/pima-indians-diabetes.data", header = FALSE) %>%
  rename(V1 = V9, V9 = V1) %>%
  mutate(V1 = as.factor(V1))

datasets <- list(sonar, hepatitis, spect, pima)
names(datasets) <- c("Sonar", "Hepatitis", "Spect", "Pima")
```

## Methods

In this section we define our methods for calculating the different metrics on a dataset for each algorithm we have used. We have ommitted the calculations for the multi-layer perceptron ANN since it is very slow. Uncommenting the below lines will produce the intended output, but will take a lot of time.

```{r}
testClassifiers <- function(dataset) {
  # The multi-layer perceptron classifier is very slow.
  # In this comparison we use the 5 epochs version to speed things up.
  # Blame backpropagation.
  algorithms <- c(
    J48,
    RF,
    SMO,
    NB,
    MLP5, # CHANGE TO MLP() AT YOUR OWN RISK!
    C45Bagging,
    AdaBoostM1)
  res <- c()
  for (func in algorithms) {
    subres <- c()
    for (i in 1:10) {
      subres <- c(
        subres, 
        list(
          calculateMetrics(
            evaluate_Weka_classifier(
              func(V1 ~ ., dataset, options = list(model = TRUE)),
              numFolds = 10))))
    }
    res <- c(res, list(subres))
  }
  names(res) <- c(
    "C4.5",
    "Random Forest",
    "SVM",
    "Naive Bayes",
    "Neural Network",
    "Bagging",
    "Boosting")
  
  res <- lapply(res, rbind_all)
  return(res)
}
```

Defined below is the method for performing pair-wise t-tests for every two algorithms for a metric on a given set of calculations.

```{r}
pairwiseTesting <- function(calcs) {
  cols <-c("Accuracy", "Precision", "Recall", "F1")
  inv <- lapply(cols, function(x) {
    sapply(calcs, "[[", i = x) %>% data.frame()
  })
  names(inv) <- cols

  lapply(inv, function(metricDF) {
    sapply(metricDF, function(var1) {
      sapply(metricDF, function(var2) {
        tmp <- t.test(var1, var2)
        paste(
          sprintf("%.3f", tmp$statistic[[1]]),
          sprintf("%.3f", tmp$p.value),
          sep = ",p=")
        #list(t = tmp$statistic[[1]], tmp$p.value)
      })
    }) %>% data.frame()
  })
}

kableMetrics <- function(results, caption) {
  results$Accuracy %>% kable(caption = paste(caption, "Accuracy")) %>% print()
  results$Precision %>% kable(caption = paste(caption, "Precision")) %>% print()
  results$Recall %>% kable(caption = paste(caption, "Recall")) %>% print()
  results$F1 %>% kable(caption = paste(caption, "F1 Score")) %>% print()
}
```

Below is the method to calculate the number of *wins* every algorithm has in a table. We say that an algorithm has one when the difference in means between it and another algorithm is in its favor with a p-value of < 0.05.

```{r}
winCalculation <- function(lst) {
  lapply(lst, function(dframe) {
    row <- sapply(dframe, function(col) {
      nums <- strsplit(as.character(col), ",p=")
      win <- sapply(nums, function(res) {
        tval <- as.numeric(res[[1]])
        pval <- as.numeric(res[[2]])
        subres <- (tval > 0) & (pval < 0.05)

        return (subres)
      })
      sum(win)
    }) %>% data.frame() %>% t() %>% data.frame()
    row.names(row) <- "Wins"
    row
  })
}
```

## Calcluations

Here we calculate our metrics resulting from 10-fold cross-validation 10 times for each (dataset, algorithm) pair and display the mean of the results.

```{r}
calculations <- lapply(datasets, testClassifiers)
```

```{r}
deepApply <- function(x, func = mean) {
  sapply(x, function(y) {
    sapply(y, func)
  })
}
metrics <- lapply(calculations, deepApply)

sapply(metrics, "[", i = "Accuracy", j = 1:6) %>% data.frame() %>% t() %>% kable(caption = "Accuracy")
sapply(metrics, "[", i = "Precision", j = 1:6) %>% data.frame() %>% t() %>% kable(caption = "Precision")
sapply(metrics, "[", i = "Recall", j = 1:6) %>% data.frame() %>% t() %>% kable(caption = "Recall")
sapply(metrics, "[", i = "F1", j = 1:6) %>% data.frame() %>% t() %>% kable(caption = "F1 Score")
```

## Comparisons

In this section we will perform pairwise t tests for each set of 10 cross-validation results of each metric in each dataset by each classifier.

### Sonar

```{r, results='asis'}
pwise.sonar <-pairwiseTesting(calculations$Sonar)
kableMetrics(pwise.sonar, "Sonar Dataset")
```

### Hepatitis

```{r, results='asis'}
pwise.hepatitis <- pairwiseTesting(calculations$Hepatitis)
kableMetrics(pwise.hepatitis, "Hepatitis Dataset")
```

### SPECT

```{r, results='asis'}
pwise.spect <- pairwiseTesting(calculations$Spect)
kableMetrics(pwise.spect, "Spect Dataset")
```

### Pima Indians

```{r, results='asis'}
pwise.pima <- pairwiseTesting(calculations$Pima)
kableMetrics(pwise.pima,"Pima Dataset")
```

## Interpretation

Here we present the win-calculations for each dataset and algorithm. The questions of who has the most wins can be answered using the below tables for each dataset and measure of efficacy. A clear and only winner is only apparent for each dataset when it accrues substantially more wins than the other algorithms it competes with.

### Sonar

```{r, results='asis'}
winCalculation(pwise.sonar) %>%
  kableMetrics("Sonar Dataset")
```

The Random Forest classifier clearly dominates wins in the sonar dataset across all measures.

### Hepatitis

```{r, results='asis'}
winCalculation(pwise.hepatitis) %>%
  kableMetrics("Hepatitis Dataset")
```

The hepatitis dataset is a close competition between the C4.5 and Naive Bayes classifiers. The C4.5 classifier performs better than all others in terms of accuracy, while the Naive Bayes classifier outperforms the others in terms of recall and F1 score. Precision appears to be equally contested by C4.5 and Bagging.

### SPECT

```{r, results='asis'}
winCalculation(pwise.spect) %>%
  kableMetrics("SPECT Dataset")
```

The Naive Bayes and SVM classifiers outperform all others in terms of recall and F1 score. Accuracy seems to be in favor of the SVM, while precision is contested by the Random Forest, Support Vector Machine, Neural Network and Bagging classifiers.

### Pima Indians

```{r, results='asis'}
winCalculation(pwise.pima) %>%
  kableMetrics("PIMA Dataset")
```

Accuracy and recall appear to be contested by both the SVM and the Neural Network. Precision seems to be a close call between all algorithms except the SVM and Boosting. The F1 score is contested by the Random Forest, SVM and Neural Network classifiers.